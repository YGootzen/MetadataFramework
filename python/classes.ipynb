{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "breeding-mediterranean",
   "metadata": {},
   "source": [
    "# Implementation of framework\n",
    "Author: Yvonne Gootzen (Statistics Netherlands & TU/e) yapm.gootzen@cbs.nl\n",
    "\n",
    "Implementation of the metadata framework with an A* algorithm based on score functions. The notebook contains:\n",
    "- Class definitions and functions (including score function), for describing the metadata problem.\n",
    "- Examples of usage of the classes to illustrate the problem.\n",
    "- A* implementation. \n",
    "\n",
    "First, import the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closed-contrast",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "#!{sys.executable} -m pip install ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optical-reset",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib as plt\n",
    "import networkx as nx\n",
    "import copy as copy\n",
    "import numpy as np\n",
    "import re\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quick-lingerie",
   "metadata": {},
   "source": [
    "# Class definitions\n",
    "This part of the notebook contains all classes and functions for describing the problem. For more information about the meanings behind each class, see the accompanying presentation and paper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "soviet-oxide",
   "metadata": {},
   "source": [
    "### Not Initialised Error\n",
    "To signal the user about situations where an object has not yet been initialised. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "common-hotel",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NotInitialisedError(Exception):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "useful-theater",
   "metadata": {},
   "source": [
    "### Variable\n",
    "The smallest object in the problem. Each dataset contains multiple variables. Variables can have different levels of granularity. To change from one granularity to another, a conversion or aggregation is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capable-tunnel",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Variable:\n",
    "    def __init__(self, name, granularity):\n",
    "        self.name = name\n",
    "        self.granularity = granularity\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(self.name) + str(self.granularity)\n",
    "    \n",
    "    def __eq__(self, other: \"Variable\"):\n",
    "        # compare self Variable object to the other Variable object\n",
    "        # they are equal if the name and granularity are equal\n",
    "        \n",
    "        return all([self.name == other.name, \n",
    "                    self.granularity == other.granularity])\n",
    "    \n",
    "    def __hash__(self):\n",
    "        # required for usage in sets. Since the str() of self is unique and contains all elements for equality, we use this for the hash.\n",
    "        return(hash(str(self)))\n",
    "    \n",
    "    def equal_name(self, other: \"Variable\"):\n",
    "        return self.name == other.name\n",
    "    \n",
    "    def get_name(self):\n",
    "        return self.name\n",
    "    \n",
    "    def get_granularity(self):\n",
    "        return self.granularity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virtual-snowboard",
   "metadata": {},
   "source": [
    "### Data\n",
    "A data source consists of a set of left-hand variables, a set of right-hand variables and context. The context is largely ignored for the modelling week. The similarity() method is the subject of the modelling week assignment. It is also used in the SetOfSources class, where the individual similarity scores of each data set are combined into a single value.\n",
    "\n",
    "The assignment of the modelling week is as follows: find a similarity score function that provides a small value for when two data sources have few variables in common, and a larger value when more variables are in common. One disadvantage of the current method is that sources with a large number of variables have a higher similarity score than sources with a smaller number of variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "narrative-tokyo",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Data(object):\n",
    "    def __init__(self, left_variables, right_variables, context, similarity_variant = \"normalized_basic\"):\n",
    "        self.left_variables = set(left_variables)\n",
    "        self.right_variables = set(right_variables)\n",
    "        self.context = set(context)  # ensure context is a set (argument may be a list)\n",
    "        self.similarity_variant = similarity_variant\n",
    "        self.lookup_table_left = None\n",
    "        self.lookup_table_right = None\n",
    "        self.lookUpTable = False\n",
    "        self.path_step = \"none\"\n",
    "\n",
    "    def __str__(self):\n",
    "        left_str_separate = [str(v) for v in self.left_variables]\n",
    "        left_str_separate.sort()\n",
    "        left_str = \", \".join(left_str_separate)\n",
    "        right_str_separate = [str(v) for v in self.right_variables]\n",
    "        right_str_separate.sort()\n",
    "        right_str = \", \".join(right_str_separate)\n",
    "        full_str = \"(\" + left_str + \" | \" + right_str + \")\" + \"_\" + str(sorted([str(c) for c in self.context]))\n",
    "        return full_str\n",
    "    \n",
    "    def __eq__(self, other: \"Data\"):\n",
    "        # compare self Data object to the other Data object\n",
    "        # they are equal if all left- and right- sets of variables, and the context, are equal\n",
    "        \n",
    "        # use all for efficient evaluation (stops at the first element that is False)\n",
    "        return all([self.left_variables == other.left_variables,\n",
    "                    self.right_variables == other.right_variables,\n",
    "                    self.context == other.context])\n",
    "    \n",
    "    def __hash__(self):\n",
    "        # required for usage in sets. Since the str() of self is unique and contains all elements for equality, we use this for the hash.\n",
    "        return hash(str(self))\n",
    "    \n",
    "    def equal_nocontext(self, other: \"Data\"):\n",
    "        # compare self Data object to the other Data object\n",
    "        # similar to __eq__() except that this function does not care about context\n",
    "        # they are considered equal if all left- and right- sets of variables, are equal\n",
    "        \n",
    "        # use all for efficient evaluation (stops at the first element that is False)\n",
    "        return all([self.left_variables == other.left_variables,\n",
    "                    self.right_variables == other.right_variables])\n",
    "    \n",
    "    def get_context(self):\n",
    "        return self.context  # set of contexts\n",
    "    \n",
    "    def get_variable_names_left(self):\n",
    "        return {v.get_name() for v in self.left_variables}\n",
    "    \n",
    "    def contains_var_left(self, v_name):\n",
    "        return v_name in self.get_variable_names_left()\n",
    "        \n",
    "    def get_variable_names_right(self):    \n",
    "        return {v.get_name() for v in self.right_variables}\n",
    "    \n",
    "    def contains_var_right(self, v_name):\n",
    "        return v_name in self.get_variable_names_right()\n",
    "    \n",
    "    def get_variables_left(self):\n",
    "        return self.left_variables\n",
    "    \n",
    "    def get_variables_right(self):\n",
    "        return self.right_variables\n",
    "    \n",
    "    def set_lookup_tables(self, lookup_table_left, lookup_table_right):\n",
    "        self.lookup_table_left = lookup_table_left\n",
    "        self.lookup_table_right = lookup_table_right\n",
    "        self.lookUpTable = True\n",
    "    \n",
    "    def convert_variable(self, var_remove, var_add):\n",
    "        # beware: the check if this conversion is allowed should be executed before this method is used\n",
    "        self.left_variables.remove(var_remove)\n",
    "        self.left_variables.add(var_add)\n",
    "        \n",
    "    def aggregate_variable(self, var_remove, var_add):\n",
    "        # beware: the check if this aggregation is allowed should be executed before this method is used\n",
    "        self.right_variables.remove(var_remove)\n",
    "        self.right_variables.add(var_add)\n",
    "    \n",
    "    def similarity(self, other: \"Data\", weight_right_sim = 1):\n",
    "        # self is the goal Data\n",
    "        \n",
    "        n_goal_vars_left = len(self.left_variables)\n",
    "        weight_right_eq = 2 * weight_right_sim\n",
    "        weight_left_sim = n_goal_vars_left * weight_right_eq\n",
    "        weight_left_eq = 2 * weight_left_sim\n",
    "        weight_context = 2 * weight_left_eq\n",
    "#         weight_left_eq, weight_left_sim, weight_right_eq = 50, 20, 10\n",
    "#         weight_context = 100\n",
    "\n",
    "        left_equal = len(set(self.left_variables).intersection(other.left_variables))  # number of variables with equal name and granularity\n",
    "        right_equal = len(set(self.right_variables).intersection(other.right_variables))  # number of variables with equal name and granularity\n",
    "        \n",
    "        if self.lookUpTable:\n",
    "            left_similar = len(self.get_variable_names_left().intersection(other.get_variable_names_left(), self.lookup_table_left))  # number of variables with equal name (those with equal granularity are counted again, so keep this in mind when setting weights)\n",
    "            right_similar = len(self.get_variable_names_right().intersection(other.get_variable_names_right(), self.lookup_table_right))\n",
    "        else:\n",
    "            left_similar = len(self.get_variable_names_left().intersection(other.get_variable_names_left()))  # number of variables with equal name (those with equal granularity are counted again, so keep this in mind when setting weights)\n",
    "            left_similar -= left_equal    # remove double-counting\n",
    "            right_similar = len(self.get_variable_names_right().intersection(other.get_variable_names_right()))  # number of variables with equal name (those with equal granularity are counted again, so keep this in mind when setting weights)\n",
    "            right_similar -= right_equal  # remove double-counting\n",
    "        \n",
    "        context_score = weight_context*(self.context == other.context)\n",
    "        \n",
    "        left_equal_max = len(set(self.left_variables))  # number of variables with equal name and granularity\n",
    "        right_equal_max = len(set(self.right_variables))  # number of variables with equal name and granularity\n",
    "\n",
    "        variant = self.similarity_variant\n",
    "        \n",
    "        if variant == 'normalized_basic':   # this is the default\n",
    "            # normalize score:\n",
    "            score = sum([weight_left_eq*left_equal, weight_left_sim*left_similar,\n",
    "                        weight_right_eq*right_equal, weight_right_sim*right_similar,\n",
    "                        context_score]) / sum([weight_left_eq * left_equal_max, weight_right_eq * right_equal_max, weight_context])\n",
    "        \n",
    "        elif variant == 'normalized_coupled':\n",
    "            # normalize score, but with rhs and lhs dependently (multiply instead of sum)\n",
    "            score = ((sum([weight_left_eq*left_equal, weight_left_sim*left_similar]) *\n",
    "                    sum([weight_right_eq*right_equal, weight_right_sim*right_similar, context_score])) /  \n",
    "                    (weight_left_eq * left_equal_max * (weight_right_eq * right_equal_max + weight_context))) \n",
    "                    \n",
    "        return score\n",
    "    \n",
    "    def get_neighbours(self, agg = True):\n",
    "        # based on conversion, aggregation and combination, give all unique datasets that can be created from self, with exactly one manipulation\n",
    "        \n",
    "        # conversion\n",
    "        neighbours = set()\n",
    "        for v in self.left_variables:\n",
    "            # for each of the left variables, it can be converted to one of its connected granularities in the conversion graph\n",
    "            conversion_graph = ConversionGraph.get(v.get_name())\n",
    "            connected_granularities = conversion_graph.all_conversions(v.get_granularity())\n",
    "            for g in connected_granularities:\n",
    "                v2 = Variable(name=v.get_name(), granularity = g)  # copy the name, but use new granularity\n",
    "                data_temp = copy.deepcopy(self)  # copy of the current data set\n",
    "                data_temp.convert_variable(var_remove = v, var_add = v2)  # apply conversion (we have checked that it is valid when creating connected_granularities)\n",
    "                data_temp.path_step = \"convert (\"+str(v) + \" to \" + str(v2) + \"): \" + str(data_temp)\n",
    "                neighbours.add(data_temp)\n",
    "        \n",
    "        # aggregation \n",
    "        if agg:\n",
    "            for v in self.right_variables:\n",
    "                # for each of the left variables, it can be converted to one of its connected granularities in the conversion graph\n",
    "                aggregation_graph = AggregationGraph.get(v.get_name())\n",
    "                connected_granularities = aggregation_graph.all_aggregations(v.get_granularity())\n",
    "                for g in connected_granularities:\n",
    "                    v2 = Variable(name=v.get_name(), granularity = g)  # copy the name, but use new granularity\n",
    "                    data_temp = copy.deepcopy(self)\n",
    "                    data_temp.aggregate_variable(var_remove = v, var_add = v2)\n",
    "                    data_temp.path_step = \"aggregate (\"+str(v) + \" to \" + str(v2) + \"): \" + str(data_temp)\n",
    "                    neighbours.add(data_temp)\n",
    "        \n",
    "        # combination is not relevant when looking at a single data source, because \n",
    "        \n",
    "        return neighbours\n",
    "    \n",
    "    def shrink(self, other: \"Data\"):  ###NEW###\n",
    "        \"\"\"\n",
    "        Returns True if self can be 'shrinked' into other. \n",
    "\n",
    "        (Temporary) solution to a combination issue where:\n",
    "\n",
    "        (a1, b3 | a3, c1)_I  +  (b2, e1 | a3, c1)_I  ->  (a1, b2, b3, e1 | a3, c1)_I\n",
    "\n",
    "        results in both b2 and b3 being in the dataset. The subdataset() function checks if a data set (goal) is within another dataset. Returns true/false. This is also relevant to the case:\n",
    "\n",
    "        (a1, b3 | a3, c1)_I  +  (b3, e1 | a3, c1)_I  ->  (a1, b3, e1 | a3, c1)_I\n",
    "\n",
    "        where the goal is a subset of the result of combining (a1, e1 | a3, c1)_I.\n",
    "\n",
    "        Left-hand variables can be dropped at any time without messing up the structure of the data set. \n",
    "        Right-hand variables can not simply be dropped. If dropped, duplicate units might exist in the \n",
    "        dataset because part of the unit descriptor is suddenly missing. Because of this, the sets of \n",
    "        right-hand variables must be equal. \n",
    "        \"\"\"\n",
    "    \n",
    "        return all([other.left_variables.issubset(self.left_variables),\n",
    "                self.right_variables == other.right_variables,\n",
    "                self.context.issubset(other.context)]) \n",
    "    \n",
    "    def shrink_nocontext(self, other: \"Data\"):  ###NEW###\n",
    "        \"\"\"\n",
    "        Same as shrink() but without the constraint that contexts must be equal\n",
    "        \"\"\"\n",
    "    \n",
    "        return all([other.left_variables.issubset(self.left_variables),\n",
    "                self.right_variables == other.right_variables])  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "careful-presentation",
   "metadata": {},
   "source": [
    "### Conversion Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pretty-symphony",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ConversionGraph:\n",
    "    instances = []  # class attribute to keep track of class instances\n",
    "    \n",
    "    def __init__(self, variable_name, granularities, conversion_edges):\n",
    "        self.variable_name = variable_name\n",
    "        self.Graph = nx.Graph()\n",
    "        self.granularities = granularities\n",
    "\n",
    "        for g in granularities:\n",
    "            self.Graph.add_node(g)\n",
    "\n",
    "        for e in conversion_edges:\n",
    "            self.Graph.add_edge(*e)  # * unpacks edge tuple\n",
    "        \n",
    "        # add self to list of instances\n",
    "        if self.is_initialised(variable_name):\n",
    "            # an instance with this variable name was already known, remove that instance (so the new instance is the only one)\n",
    "            ConversionGraph.instances.remove(self.get(variable_name))\n",
    "        ConversionGraph.instances.append(self)  # append instance to list of class instances\n",
    "        \n",
    "    @classmethod \n",
    "    def get(cls: \"ConversionGraph\", var_name):\n",
    "        # return the instance of this class for which the value variable_name is equal to var_name\n",
    "        # each ConversionGraph object should exist exactly once for each variable_name\n",
    "        \n",
    "        # first we make a list of \"all\" instances that statisfy the desired variable name\n",
    "        \n",
    "        list_form = [inst for inst in cls.instances if inst.variable_name == var_name]\n",
    "        if len(list_form) > 0:\n",
    "            # list is not empty, so return the first element (there should only be one)\n",
    "            return list_form[0]\n",
    "        else:\n",
    "            # no instance was found\n",
    "            raise NotInitialisedError(\"ConversionGraph \" + var_name)\n",
    "            \n",
    "    def get_max_granularities(self):\n",
    "        return max(self.granularities)\n",
    "        \n",
    "    def is_initialised(cls: \"ConversionGraph\", var_name):\n",
    "        list_form = [inst for inst in cls.instances if inst.variable_name == var_name]\n",
    "        return len(list_form) > 0\n",
    "\n",
    "    def add_granularity(self, new_granularity):\n",
    "        self.Graph.add_node(new_granularity)\n",
    "\n",
    "    def add_conversion_edge(self, new_edge):\n",
    "        self.Graph.add_edge(*new_edge)\n",
    "\n",
    "    def plot_graph(self):\n",
    "        nx.draw(self.Graph, with_labels=True, node_color=\"lightgrey\")\n",
    "        \n",
    "    def check_conversion(self, granularity_from, granularity_to):\n",
    "        # true: if there is a conversion path between granularity_from to granularity_to\n",
    "        # false: otherwise\n",
    "        return nx.has_path(self.Graph, granularity_from, granularity_to)\n",
    "    \n",
    "    def all_conversions(self, granularity_from):\n",
    "        # returns all possible granularities that can be reached from the granularity_from\n",
    "        connected_set = nx.node_connected_component(self.Graph, granularity_from)  # this includes the starting node\n",
    "        connected_set.remove(granularity_from)  # exclude starting node\n",
    "        return connected_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tested-preview",
   "metadata": {},
   "source": [
    "### Aggregation Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hindu-invention",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AggregationGraph:\n",
    "    instances = []  # class attribute to keep track of class instances\n",
    "    \n",
    "    def __init__(self, variable_name, granularities, aggregation_edges):\n",
    "        self.variable_name = variable_name\n",
    "        self.Graph = nx.DiGraph()\n",
    "        self.granularities = granularities\n",
    "\n",
    "        for g in granularities:\n",
    "            self.Graph.add_node(g)\n",
    "\n",
    "        for e in aggregation_edges:\n",
    "            self.Graph.add_edge(*e)  # * unpacks edge tuple\n",
    "            \n",
    "         # add self to list of instances\n",
    "        if self.is_initialised(variable_name):\n",
    "            # an instance with this variable name was already known, remove that instance (so the new instance is the only one)\n",
    "            AggregationGraph.instances.remove(self.get(variable_name))\n",
    "        AggregationGraph.instances.append(self)  # append instance to list of class instances\n",
    "            \n",
    "    @classmethod \n",
    "    def get(cls: \"AggregationGraph\", var_name):\n",
    "        # return the instance of this class for which the value variable_name is equal to var_name\n",
    "        # each AggregationGraph object should exist exactly once for each variable_name\n",
    "        \n",
    "        # first we make a list of \"all\" instances that statisfy the desired variable name\n",
    "        list_form = [inst for inst in cls.instances if inst.variable_name == var_name]\n",
    "        if len(list_form) > 0:\n",
    "            # list is not empty, so return the first element (there should only be one)\n",
    "            return list_form[0]\n",
    "        else:\n",
    "            # no instance was found\n",
    "            raise NotInitialisedError(\"AggregationGraph \" + var_name)\n",
    "            \n",
    "    def is_initialised(cls: \"AggregationGraph\", var_name):\n",
    "        list_form = [inst for inst in cls.instances if inst.variable_name == var_name]\n",
    "        return len(list_form) > 0\n",
    "\n",
    "    def get_max_granularities(self):\n",
    "        return max(self.granularities)        \n",
    "\n",
    "    def add_granularity(self, new_granularity):\n",
    "        self.Graph.add_node(new_granularity)\n",
    "\n",
    "    def add_conversion_edge(self, new_edge):\n",
    "        self.Graph.add_edge(*new_edge)  # * unpacks edge tuple\n",
    "\n",
    "    def plot_graph(self): \n",
    "        nx.draw(self.Graph, with_labels=True, node_color=\"lightgrey\")\n",
    "        \n",
    "    def check_aggregation(self, granularity_from, granularity_to):\n",
    "        # true: if there is an aggregation path between granularity_from to granularity_to\n",
    "        # false: otherwise\n",
    "        return nx.has_path(self.Graph, granularity_from, granularity_to)\n",
    "    \n",
    "    def all_aggregations(self, granularity_from):\n",
    "        # returns all possible granularities that can be reached from the granularity_from\n",
    "        reacheable_set = nx.descendants(self.Graph, granularity_from) \n",
    "        \n",
    "        return reacheable_set\n",
    "        \n",
    "    def all_aggregations_reversed(self, granularity_to):\n",
    "        # returns all possible granularities that can be reached from the granularity_from\n",
    "        reacheable_set = nx.ancestors(self.Graph, granularity_to) \n",
    "        \n",
    "        return reacheable_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decimal-silence",
   "metadata": {},
   "source": [
    "### Combining \n",
    "Combining depends on two data sources. It is allowed when their right variables are equal. Column wise combining: when the contexts of both input sources have some overlap (non-empty intersection), the union of all left variables is available in the new data source, for the intersection of the context. Row wise combining: when the contexts of both sources have no overlap (empty intersection), the intersection of the left variables is available in the new data source, but the new context is the union of the context of the two input sources. Sometimes, both row-wise and column-wise combination are possible, resulting into two different outcomes. The combines() function checks both options and will return a tuple with te row- and column-wise combination results respectively. If a combination is not possible, the result will be False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "separated-chemical",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combines(data1: Data, data2: Data):\n",
    "    # Create rowwise and colwise (in case combinations are not possible)\n",
    "    rowwise, colwise = False, False\n",
    "    \n",
    "    # Combinations are only possible if the right-hand side variables are equal\n",
    "    if data1.right_variables == data2.right_variables:\n",
    "        # The result will have the same right-hand side variables \n",
    "        right3 = data1.right_variables\n",
    "       \n",
    "        # row-wise combination\n",
    "        if set(data1.left_variables) & set(data2.left_variables):\n",
    "            # there is overlap between the left variables \n",
    "            \n",
    "            # row-wise merge possible     \n",
    "            # no overlap between context of both sources, so only the same left-hand side variables can be merged\n",
    "            left3 = set(data1.left_variables).intersection(set(data2.left_variables))  # intersection of L1 and L2\n",
    "            context3 = data1.context.union(data2.context)  # union of C1 and C3\n",
    "            \n",
    "            rowwise = Data(right_variables = right3, left_variables = left3, context = context3)\n",
    "            \n",
    "        # column-wise combination \n",
    "        if data1.context & data2.context:\n",
    "            # set1 & set2: checks if there exists an intersection between two sets\n",
    "        \n",
    "            # column-wise merge possible\n",
    "            # context of both input sources have overlap, so the left-hand variables can be merged\n",
    "            left3 = set(data1.left_variables).union(set(data2.left_variables))  # union of L1 and L2\n",
    "            context3 = data1.context.intersection(data2.context)  # intersection of C1 and C2\n",
    "            \n",
    "            colwise = Data(right_variables = right3, left_variables = left3, context = context3)\n",
    "        \n",
    "    return rowwise, colwise\n",
    "        \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mental-machinery",
   "metadata": {},
   "source": [
    "### Set of Sources\n",
    "Two variants of the similarity score function are implemented: the sum and max of the individual data scores from the data source similarity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "finished-array",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mSetOfSources\u001b[39;00m:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, start_set):\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_of_sources \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(start_set)\n",
      "Cell \u001b[0;32mIn[1], line 15\u001b[0m, in \u001b[0;36mSetOfSources\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__eq__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# check if this set equals the other set\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# since the set contains Data objects, which have a __eq__() method, it suffices to rely on that method\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# considered equal if the sets have the data Data objects, regardless of order in which the Data objects appear\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_of_sources \u001b[38;5;241m==\u001b[39m other\u001b[38;5;241m.\u001b[39mset_of_sources\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd_data_source\u001b[39m(\u001b[38;5;28mself\u001b[39m, data_new: \u001b[43mData\u001b[49m):\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_of_sources \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_of_sources\u001b[38;5;241m.\u001b[39munion({data_new})\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcontains\u001b[39m(\u001b[38;5;28mself\u001b[39m, data_source: Data): \n",
      "\u001b[0;31mNameError\u001b[0m: name 'Data' is not defined"
     ]
    }
   ],
   "source": [
    "class SetOfSources:\n",
    "    def __init__(self, start_set):\n",
    "        self.set_of_sources = set(start_set)\n",
    "        self.path = [\"start_set\"]  # for keeping track of the path that created the current set\n",
    "        self.tree = []  # for keeping track of which iterations of the algorithm added to this path\n",
    "        \n",
    "    def __str__(self):\n",
    "        full_str = \"{\" + \",\\n \".join([str(d) for d in self.set_of_sources]) + \"\\n}\"\n",
    "        return full_str\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        # check if this set equals the other set\n",
    "        # since the set contains Data objects, which have a __eq__() method, it suffices to rely on that method\n",
    "        # considered equal if the sets have the same data Data objects, \n",
    "        # regardless of order in which the Data objects appear\n",
    "        return self.set_of_sources == other.set_of_sources\n",
    "    \n",
    "    def add_data_source(self, data_new: Data, part_of_path, iteration):\n",
    "        self.set_of_sources = self.set_of_sources.union({data_new})\n",
    "        self.add_to_path(part_of_path)\n",
    "        self.tree.append(iteration)\n",
    "        \n",
    "    def add_to_path(self, part_of_path: str):\n",
    "        # For keeping track of the path (in words).\n",
    "        self.path.append(part_of_path)\n",
    "        \n",
    "    def contains(self, data_source: Data): \n",
    "        if data_source in self.set_of_sources:\n",
    "            return True\n",
    "        else:\n",
    "            # check if any of the data sources in self can be shrinked into data_source\n",
    "            # this takes longer to compute, so only do this step when data_source is not exactly in self\n",
    "            return any([data_in_self.shrink(data_source) for data_in_self in self.set_of_sources])\n",
    "        \n",
    "    def contains_nocontext(self, data_source: Data): ###NEW### (updated)\n",
    "        # Same as contains() except here, we do NOT care about the context \n",
    "        # This is used for some models\n",
    "        \n",
    "        matching_sources = [ds for ds in self.set_of_sources if ds.equal_nocontext(data_source)]\n",
    "        \n",
    "        if data_source in self.set_of_sources:\n",
    "            return matching_sources\n",
    "        else:\n",
    "            # check if any of the data sources in self can be shrinked into data_source\n",
    "            # this takes longer to compute, so only do this step when data_source is not exactly in self\n",
    "            \n",
    "            matching_sources = [data_in_self for data_in_self in self.set_of_sources if data_in_self.shrink_nocontext(data_source)]\n",
    "            return matching_sources\n",
    "        \n",
    "        \n",
    "    def data_sources_with_var_left(self, v_name):\n",
    "        # TODO this code (from the students) can be sped up by list comprehension\n",
    "        data_sources = set()\n",
    "        \n",
    "        for d in self.set_of_sources:\n",
    "            if d.contains_var_left(v_name):\n",
    "                data_sources.add(d)\n",
    "                \n",
    "        return data_sources\n",
    "    \n",
    "    def data_sources_with_var_right(self, v_name):\n",
    "        data_sources = set()\n",
    "        \n",
    "        for d in self.set_of_sources:\n",
    "            if d.contains_var_right(v_name):\n",
    "                data_sources.add(d)\n",
    "                \n",
    "        return data_sources\n",
    "            \n",
    "    def similarity_sum(self, goal_data: Data):\n",
    "        return sum((goal_data.similarity(d) for d in self.set_of_sources))  \n",
    "    \n",
    "    def similarity_max(self, goal_data: Data):\n",
    "        return max((goal_data.similarity(d) for d in self.set_of_sources)) \n",
    "    \n",
    "    def similarity_mean(self, goal_data: Data):\n",
    "        return np.mean(list(goal_data.similarity(d) for d in self.set_of_sources))\n",
    "\n",
    "    def similarity_median(self, goal_data: Data):\n",
    "        return np.median(list(goal_data.similarity(d) for d in self.set_of_sources))\n",
    "    \n",
    "    def similarity_min(self, goal_data: Data):\n",
    "        return min((goal_data.similarity(d) for d in self.set_of_sources))\n",
    "    \n",
    "    def similarity_minmax(self, goal_data: Data):\n",
    "        return max((goal_data.similarity(d) for d in self.set_of_sources)) * (min((goal_data.similarity(d) for d in self.set_of_sources)))\n",
    "    \n",
    "    def similarity_maxmean(self, goal_data: Data):\n",
    "        return max((goal_data.similarity(d) for d in self.set_of_sources)) + np.mean(list(goal_data.similarity(d) for d in self.set_of_sources))\n",
    "    \n",
    "    def similarity_maxmeanmin(self, goal_data: Data):\n",
    "        return max((goal_data.similarity(d) for d in self.set_of_sources)) * np.mean(list(goal_data.similarity(d) for d in self.set_of_sources)) * min((goal_data.similarity(d) for d in self.set_of_sources))\n",
    "    \n",
    "    def similarity_max_per_variable(self, goal_data: Data):        \n",
    "        idx = 0\n",
    "        \n",
    "        goal_contexts = goal_data.get_context()\n",
    "        maxs = np.zeros(len(goal_data.left_variables)*len(goal_contexts))\n",
    "        \n",
    "        for context in goal_contexts:\n",
    "            \n",
    "            for var in goal_data.left_variables:\n",
    "                data_sources = self.data_sources_with_var_left(var.get_name())\n",
    "                \n",
    "                if data_sources:\n",
    "                    maxs[idx] = max((goal_data.similarity(d) for d in data_sources if context in d.get_context()))\n",
    "                idx += 1\n",
    "            \n",
    "        return np.mean(maxs)\n",
    "    \n",
    "    def similarity_max_per_variable_bonus(self, goal_data: Data):\n",
    "        # Reimplemented from the students' version, disregarding context for now\n",
    "        \n",
    "        # TODO: check all context-related code by the students, this was not well-defined,\n",
    "        # It seems context was assumed to be a list. It is now set-based. \n",
    "        \n",
    "        # TODO: check this with the report and reimplement \n",
    "        idx = 0\n",
    "        \n",
    "        maxs = np.zeros(len(goal_data.left_variables))\n",
    "        maxs_data_sources = []\n",
    "        \n",
    "        for left_var in goal_data.left_variables:\n",
    "            # get all data sources in current_set that have the same left variable\n",
    "            data_sources_left_match = list(self.data_sources_with_var_left(left_var.get_name()))\n",
    "\n",
    "            if len(data_sources_left_match) > 0:\n",
    "                # one or more data sources were found\n",
    "                \n",
    "                scores_list = [goal_data.similarity(d) for d in data_sources_left_match]\n",
    "                max_idx = np.argmax(scores_list)\n",
    "                max_data_source = data_sources_left_match[max_idx]\n",
    "                maxs_data_sources.append(max_data_source.right_variables)\n",
    "                maxs[idx] = scores_list[max_idx]\n",
    "            idx += 1\n",
    "                \n",
    "        bonus_mult = len(set.intersection(*maxs_data_sources)) / len(set.union(*maxs_data_sources))\n",
    "        \n",
    "        return np.mean(maxs) * (bonus_mult + 1)/2    # don't start at 0\n",
    "                        \n",
    "    def get_sources(self):\n",
    "        return self.set_of_sources\n",
    "    \n",
    "    def get_neighbours(self, agg = True):\n",
    "        # based on conversion, aggregation and combination, give all unique datasets that can be created from the current set, with exactly one manipulation\n",
    "        # since combination often leads quicker to results, we start by combination\n",
    "        \n",
    "        all_neighbours = set()\n",
    "        \n",
    "        # Conversion and aggregating\n",
    "        for d in self.set_of_sources:\n",
    "            # add all items in the set d.get_neighbours() to all_neighbours\n",
    "            # These neighbours come from the individual datasets (and already have their path noted). \n",
    "            all_neighbours.update(d.get_neighbours(agg))\n",
    "        \n",
    "        # Combination\n",
    "        set_of_sources_temp = list(self.set_of_sources)  # temporarily make the set of sources into a list, so the indices are fixed\n",
    "        \n",
    "        for i, j in zip(*np.triu_indices(len(set_of_sources_temp), k=1)):\n",
    "            # Loop through all combinations of (two) available data sources by using the indices of the upper \n",
    "            # triangle without diagonal (offset k=1) of a matrix of size n by n, where n = len(self.set_of_sources)\n",
    "        \n",
    "            combines_temp_row, combines_temp_col = combines(set_of_sources_temp[i], set_of_sources_temp[j])\n",
    "            if combines_temp_row:\n",
    "                # Rowwise combination was possible, so add result to neighbours\n",
    "                combines_temp_row.path_step = \"combine (rowwise): \" + str(combines_temp_row)\n",
    "                all_neighbours.add(combines_temp_row)  \n",
    "                \n",
    "            if combines_temp_col:\n",
    "                # Columnwise combination was possible, so add result to neighbours\n",
    "                combines_temp_col.path_step = \"combine (columnwise): \" + str(combines_temp_col)\n",
    "                all_neighbours.add(combines_temp_col) \n",
    "                        \n",
    "        return all_neighbours\n",
    "    \n",
    "\n",
    "    def get_neighbours_models(self, models = None):\n",
    "        # based on modelling, give all unique datasets that can be created from the current set, with exactly one manipulation\n",
    "        # since combination often leads quicker to results, we start by combination\n",
    "        \n",
    "        all_neighbours = set()\n",
    "        \n",
    "        # Modelling\n",
    "        if models is not None:\n",
    "            # Since the case study only uses models with exactly two input sources, restrict the combinations\n",
    "            # to exactly two sources. The code below allows for all combinations of any number, but this increases\n",
    "            # the running time.\n",
    "            \n",
    "            # Get all (unique) subsets of all available sources (like a powerset, except for the emptyset and single sources)\n",
    "            #data_combos = list(subset_combos(set_of_sources_temp))\n",
    "            #for dataset_combo in data_combos:  # (then use this instead of the (i, j) loop)\n",
    "            \n",
    "            set_of_sources_temp = list(self.set_of_sources)  # temporarily make the set of sources into a list, so the indices are fixed\n",
    "        \n",
    "            for i, j in zip(*np.triu_indices(len(set_of_sources_temp), k=1)):\n",
    "                # Loop through all combinations of (two) available data sources by using the indices of the upper \n",
    "                # triangle without diagonal (offset k=1) of a matrix of size n by n, where n = len(self.set_of_sources)\n",
    "\n",
    "                # Loop through all combinations of available data sources and see if any models are applicable\n",
    "                for model_tmp in models:\n",
    "                    # If the model is applicable, the list of output sources will be returned. Note: multiple outputs\n",
    "                    # are possible. \n",
    "                    model_output = model_tmp.apply(potential_input = [set_of_sources_temp[i], set_of_sources_temp[j]])\n",
    "                    if model_output != False:\n",
    "                        # The model was applicable and returned output. Add this output to the list of all neighbours.\n",
    "                        # When unapplicable the value of model_output is False.\n",
    "                        for mo in model_output:\n",
    "                            mo.path_step = \"model (\" + model_tmp.name + \"):\" + str(mo)\n",
    "                \n",
    "                        all_neighbours.update(set(model_output))\n",
    "                        \n",
    "        return all_neighbours\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alternate-choir",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestCase:\n",
    "    def __init__(self, goal, start_set, graphs, models=None):\n",
    "        self.goal = goal\n",
    "        self.start_set = start_set\n",
    "        self.graphs = graphs\n",
    "        self.models = models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "matched-shoulder",
   "metadata": {},
   "source": [
    "### Modelling\n",
    "Modelling is currently implemented as exceptions to the rules of the other manipulations. Each available model, however trivial, must be specified. A model is based on a set of input data and output data. If the input data is available, then the output data can be acchieved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brilliant-tiffany",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, input_data, output_data, context_rule):\n",
    "        self.input_data = set(input_data)  # can be multiple data sources\n",
    "        self.output_data = output_data\n",
    "        self.context_rule = context_rule\n",
    "            \n",
    "    def apply(self, potential_input):\n",
    "        # Note: many context rules can be thought of. If they get so specific that the relation between sources and context matter, \n",
    "        # the intention is to write a child class that overwrites the apply() function for the specific case\n",
    "            \n",
    "        # If each source in the required input (self.input_data) is present in the potential_input, then the model is applicable\n",
    "        # It gets a bit tricky to check this, because sometimes, the context is less strictly required then other times\n",
    "        \n",
    "        if self.context_rule == \"exact\":\n",
    "            # Check if the all input sources are available in the potential input, with exact context matches\n",
    "            \n",
    "            if all([input_source in potential_input for input_source in self.input_data]):\n",
    "                # based on exact matches\n",
    "                return [self.output_data]\n",
    "            else:\n",
    "                # if for all required data sources, there is a source in the potential data that can be shrinked into the required data source, \n",
    "                # the model can also be applied\n",
    "                results = []\n",
    "                for required_data in self.input_data:\n",
    "                    results.append(any([pi.shrink(required_data) for pi in potential_input]))\n",
    "                    \n",
    "                # check if shrinking was enough to satisfy requirements\n",
    "                if all(results):\n",
    "                    return [self.output_data]\n",
    "                else:\n",
    "                    return False\n",
    "        \n",
    "        elif self.context_rule in [\"intersection\", \"union\", \"equal\"]:\n",
    "            # For these context_rule's we'll need to do some set manipulation to find out if the model requirements are met\n",
    "            \n",
    "            output_list = []  # here we will add any outcomes for the inputs that satisfy the context requirements\n",
    "            context_matches = []  # list of lists \n",
    "            \n",
    "            for input_data_temp in self.input_data:\n",
    "                # for each required input_data source, check if there are matches in potential_input. If so, add their contexts to the list.\n",
    "                context_matches_temp = [ds.get_context() for ds in potential_input if ds.equal_nocontext(input_data_temp)]\n",
    "                context_matches.append(context_matches_temp)\n",
    "            \n",
    "            if all(len(context_matches_temp)>0 for context_matches_temp in context_matches):\n",
    "                # For all required input sources, at least one available data source was found\n",
    "                \n",
    "                for context_permutation in itertools.product(*context_matches):\n",
    "                    # Note: * unpacks the list context_matches once. This results in context_permutations being a tuple of the same length\n",
    "                    # as the number of required input sources.\n",
    "                    \n",
    "                    if self.context_rule == \"intersection\":\n",
    "                        # The context of the output_data is the intersection of all contexts of the input data. \n",
    "                        # If no intersection is possible, the model cannot be applied\n",
    "            \n",
    "                        # Use * to unpack the tuple context_permutaiton, and then we calculate the intersection of the various contexts\n",
    "                        context_intersection = set.intersection(*context_permutation)\n",
    "                        if len(context_intersection) > 0:\n",
    "                            # If this resulted in a non-empty set, we have found a permutation that works! \n",
    "                            new_context = context_intersection\n",
    "                            \n",
    "                    elif self.context_rule == \"union\":\n",
    "                        # Use * to unpack the tuple context_permutaiton, and then we calculate the union of the various contexts\n",
    "                        context_union = set.union(*context_permutation)\n",
    "                        if len(context_union) > 0:\n",
    "                            # If this resulted in a non-empty set, we have found a permutation that works! \n",
    "                            new_context = context_union\n",
    "                            \n",
    "                    elif self.context_rule == \"equal\":\n",
    "                        context_union = set.union(*context_permutation)  # calculate union of all context\n",
    "                        \n",
    "                        if all([cp == context_union for cp in context_permutation]):\n",
    "                            # If the union of all sets are equal to all of the sets, then all sets are equal\n",
    "                            new_context = context_union\n",
    "        \n",
    "                    # Now, we can generate an output of the model.\n",
    "                    output_data_temp = copy.deepcopy(self.output_data)  # copy the output_data\n",
    "                    output_data_temp.context = new_context  # overwrite the context \n",
    "                    output_list.append(output_data_temp)\n",
    "\n",
    "                if len(output_list) > 0:\n",
    "                    # One ore more results were found, these can now be returned\n",
    "                    return output_list\n",
    "            else:\n",
    "                return False\n",
    "                 \n",
    "        else: \n",
    "            print(\"Modelling error: the rules regarding context were not clear\")\n",
    "            return False\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bab131f",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b778e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_combos(s):\n",
    "    # start at range 2 because we do not need the emptyset or single sources\n",
    "    return [combo for r in range(2, len(s) + 1) for combo in itertools.combinations(s, r)]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
