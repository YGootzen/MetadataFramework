{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A* implementation\n",
    "This notebook contains an implementation of the A* algorithm. It is based on the similarity score function of the set of available data sources, which is in turn based on (a combination of) the similarity score function for data sources. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from ipynb.fs.full.classes import *  # import classes\n",
    "import time as time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions\n",
    "These functions were (partly) created by the modelling week students to boost the performance of A-Star. They may also be placed in classes, but since they don't really hold any logic of the metadata framework, they have been placed here instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(similarity_choice, open_list, goal, variant=\"base\", prints=False, score_function_parameter=None):\n",
    "    # from all possible variants for the available set of data sources, take the one with the highest similarity score\n",
    "    if similarity_choice == \"sum\":\n",
    "        all_scores = [temp_set.similarity_sum(goal, variant=variant) for temp_set in open_list]\n",
    "    elif similarity_choice == \"topsum\":\n",
    "        all_scores = [temp_set.similarity_topsum(goal, variant=variant, prints=prints, multiplier=score_function_parameter) for temp_set in open_list]\n",
    "    elif similarity_choice == \"max\":\n",
    "        all_scores = [temp_set.similarity_max(goal, variant=variant) for temp_set in open_list]\n",
    "    elif similarity_choice == \"mean\":\n",
    "        all_scores = [temp_set.similarity_mean(goal, variant=variant) for temp_set in open_list]\n",
    "    elif similarity_choice == \"median\":\n",
    "        all_scores = [temp_set.similarity_median(goal, variant=variant) for temp_set in open_list]\n",
    "    elif similarity_choice == \"min\":\n",
    "        all_scores = [temp_set.similarity_min(goal, variant=variant) for temp_set in open_list]\n",
    "    elif similarity_choice == \"minmax\":\n",
    "        all_scores = [temp_set.similarity_minmax(goal, variant=variant) for temp_set in open_list]\n",
    "    elif similarity_choice == \"maxmean\":\n",
    "        all_scores = [temp_set.similarity_maxmean(goal, variant=variant) for temp_set in open_list]\n",
    "    elif similarity_choice == \"maxmeanmin\":\n",
    "        all_scores = [temp_set.similarity_maxmeanmin(goal, variant=variant) for temp_set in open_list]\n",
    "    elif similarity_choice == \"max_per_variable\":\n",
    "        all_scores = [temp_set.similarity_max_per_variable(goal, variant=variant) for temp_set in open_list]\n",
    "    elif similarity_choice == \"max_per_variable_bonus\":\n",
    "        all_scores = [temp_set.similarity_max_per_variable_bonus(goal, variant=variant) for temp_set in open_list]\n",
    "    else: \n",
    "        print(\"No known similarity score option was chosen\")\n",
    "        return False\n",
    "    \n",
    "    return all_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function prep_rhs() takes the starting set of available sources and will for each source, aggregate it as far as possible to match the right hand side of the end goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_rhs(start_set, goal):\n",
    "    start_set_copy = copy.deepcopy(start_set)\n",
    "    vars_left = []   # all lhs variables in start_set with a rhs that can be aggregated to the goal rhs\n",
    "\n",
    "    for data_source in start_set.set_of_sources:\n",
    "        # for each of the data sources, we'll check if we can aggregate (some of) it's rhs variables to match the goal rhs\n",
    "        data_source_new = copy.deepcopy(data_source)\n",
    "\n",
    "        for v_r in data_source.right_variables:\n",
    "            # check if one of the goal rhs variables can be reached by this variable\n",
    "            \n",
    "            aggregation_graph = AggregationGraph.get(v_r.get_name())  # loop up corresponding aggregation graph\n",
    "            connected_granularities = aggregation_graph.all_aggregations(v_r.get_granularity())\n",
    "            for g in connected_granularities:\n",
    "                v2 = Variable(name=v_r.get_name(), granularity = g)  # copy the name, but use the new granularity\n",
    "\n",
    "                if v2 in goal.right_variables:\n",
    "                    # a rhs variable of the goal is reached! \n",
    "                    data_source_new.aggregate_variable(var_remove = v_r, var_add = v2)\n",
    "                    # there should only be one (since variable names should only occur once in the rhs)\n",
    "                    break # step out of the v_r for loop\n",
    "        if data_source_new!=data_source:\n",
    "            # some change has been made by the rhs preprocessing\n",
    "            # once all variables have been checked, we can add the data_source_new to the new start_set\n",
    "            start_set_copy.add_data_source(data_source_new, part_of_path=\"prep_rhs: \"+str(data_source)+\" -> \"+str(data_source_new))\n",
    "\n",
    "    return start_set_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A* algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def a_star(start_set, goal, models, max_iteration, similarity_choice = \"sum\", prints=False, \n",
    "          preprocess_rhs = False, find_multiple_paths=False, shedding=False, shedding_n = 10, variant=\"base\", score_function_parameter=None):\n",
    "\n",
    "    if prints: print(\"Starting A* function, goal:\" + str(goal))\n",
    "    \n",
    "    # initialize open and closed lists\n",
    "    open_list = []\n",
    "    closed_list = []\n",
    "    success_list = []\n",
    "    current_set = start_set    # for printing update\n",
    "    previous_score = -1\n",
    "    \n",
    "    # Preprocessing: check for all rhs of data sources if they can be aggregated towards the goal rhs\n",
    "    if  preprocess_rhs:\n",
    "        # First make right-hand side variables of the start_set correspond to the goal, or terminate when \n",
    "        # this is not possible\n",
    "        if preprocess_rhs:\n",
    "            start_set_copy = prep_rhs(start_set, goal)\n",
    "            agg = False  # aggregation was prepared via prep_rhs() so give it zero priority until algorithm is completely stuck\n",
    "            open_list.append(start_set_copy)  # add start node\n",
    "\n",
    "            if prints:\n",
    "                print(\"Preprocessed starting set of sources into: \"+str(start_set_copy))\n",
    "            \n",
    "    else:\n",
    "        agg = True\n",
    "        open_list.append(start_set)  # add start node\n",
    "    \n",
    "    if prints:\n",
    "        print(\"Starting A* search.\")\n",
    "        \n",
    "    # loop until we find the desired data set\n",
    "    for i in range(max_iteration):\n",
    "        if prints:\n",
    "            print(\"--- Iteration \"+str(i)+\" ---\")\n",
    "            print(\"   Length open list: \"+ str(len(open_list)))\n",
    "            print(\"   Length closed list: \"+ str(len(closed_list)))\n",
    "           \n",
    "        if i > 0:\n",
    "            previous_score = all_scores[current_index]\n",
    "        \n",
    "        if len(open_list) == 0:\n",
    "            # If the open_list is completely empty, the algorithm has failed to find (the next) succesful path.\n",
    "            \n",
    "            if find_multiple_paths:\n",
    "                if len(success_list) > 0:\n",
    "                    return success_list\n",
    "            \n",
    "            end_message = \"Open list was empty. Ran for \" + str(i) + \" iterations.\"\n",
    "            if shedding:\n",
    "                end_message += \" Shedding was used for \"+str(shedding_n)+ \" best branches. You could try again with more branches or no shedding.\"\n",
    "            else: \n",
    "                end_message += \" No more solutions will be found.\"\n",
    "\n",
    "            return end_message\n",
    "        \n",
    "        # from all possible variants for the available set of data sources, take the one with the \n",
    "        # highest similarity score\n",
    "        all_scores = get_scores(similarity_choice, open_list, goal, variant=variant, prints=prints, score_function_parameter=score_function_parameter)\n",
    "        \n",
    "        # update current set\n",
    "        current_index = all_scores.index(max(all_scores))  # find highest scoring index\n",
    "        current_set = open_list[current_index]  # take the element with highest score\n",
    "        current_score = all_scores[current_index]\n",
    "       \n",
    "        # (optional for speed up) keep only the best options in the open list\n",
    "        # this speeds up the search, but may lose potential solutions\n",
    "        if shedding & (len(open_list)>shedding_n):\n",
    "            top_indices = np.array(all_scores).argsort()[-shedding_n:][::-1]  # find highest scoring indices\n",
    "            open_list_new = [open_list[i] for i in top_indices]   # take the element with highest score\n",
    "            open_list = open_list_new\n",
    "\n",
    "        # pop current set off of the open list and add it to closed list\n",
    "        try: \n",
    "            open_list.remove(current_set) \n",
    "        except ValueError: \n",
    "            # when shedding, it may occur that the current_set was already removed by the shedding\n",
    "            pass\n",
    "        closed_list.append(current_set)\n",
    "       \n",
    "        if prints:\n",
    "            print(\"   Score of current set: \" + str(current_score))\n",
    "            #print(\"   Current set: \\n\" + str(current_set))\n",
    "            print(\"   Current set size: \" + str(len(current_set.get_sources())))\n",
    "            print(\"   Path length: \" + str(len(current_set.path)))\n",
    "            print(\"   Current path: \" + str(current_set.path))\n",
    "          \n",
    "        # check if the goal has been reached\n",
    "        if(current_set.contains(goal)):\n",
    "            if find_multiple_paths:\n",
    "                success_list.append(current_set)\n",
    "            else:\n",
    "                return current_set\n",
    "        \n",
    "        # Identify all neighbours\n",
    "        n_neighbours_model = 0\n",
    "        n_neighbours_nonmodel = 0\n",
    "        all_neighbours_mod = current_set.get_neighbours_models(models=models)  # only modelling\n",
    "        \n",
    "        # Modelling \n",
    "        # If modelling is possible, we will try this first (it is usually a good idea to prioritise this)\n",
    "        for neighbour in all_neighbours_mod:\n",
    "            # each neighbour of the current set can be created and added to the set\n",
    "            new_set_tmp = copy.deepcopy(current_set)\n",
    "            new_set_tmp.add_data_source(neighbour, neighbour.path_step, i)  \n",
    "\n",
    "            if (new_set_tmp not in open_list) and (new_set_tmp not in closed_list):\n",
    "                # the new set is not already waiting to be evaluated (open_list) and has also not been \n",
    "                # evaluated yet (closed_list)\n",
    "                open_list.append(new_set_tmp)\n",
    "                n_neighbours_model += 1\n",
    "        \n",
    "        if n_neighbours_model == 0:\n",
    "            # No models led to new results. So we will now check if combination, aggregation and conversion can be applied\n",
    "            all_neighbours_reg = current_set.get_neighbours(agg=agg)  # except modelling (and depending on agg, perhaps also without aggregation)\n",
    "\n",
    "            if agg==False & len(all_neighbours_reg)==0:\n",
    "                # if without aggregation there were no neighbours found, we will now try once with aggregation\n",
    "                all_neighbours_reg = current_set.get_neighbours(agg=True)  # except modelling\n",
    "\n",
    "            for neighbour in all_neighbours_reg:\n",
    "                # each neighbour of the current set can be created and added to the set\n",
    "                new_set_tmp = copy.deepcopy(current_set)\n",
    "                new_set_tmp.add_data_source(neighbour, neighbour.path_step, i)  \n",
    "\n",
    "                if (new_set_tmp not in open_list) and (new_set_tmp not in closed_list):\n",
    "                    # the new set is not already waiting to be evaluated (open_list) and has also not \n",
    "                    # been evaluated yet (closed_list)\n",
    "                    open_list.append(new_set_tmp)\n",
    "                    n_neighbours_nonmodel += 1\n",
    "        \n",
    "        if prints:\n",
    "            print(\"   New neighbours: \" + str(n_neighbours_model + n_neighbours_nonmodel)\n",
    "                + \" (model: \" + str(n_neighbours_model) + \", non-model: \"+str(n_neighbours_nonmodel)+\")\")\n",
    "\n",
    "    if find_multiple_paths:\n",
    "        return success_list\n",
    "    else:\n",
    "        return {\"Did not finish within \" + str(max_iteration) + \" iterations.\"}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulate\n",
    "For running the A* multiple times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate(n_simulations, start_set, goal, models, max_iteration, similarity_choice = \"sum\",\n",
    "          preprocess_rhs = False, shedding=False, shedding_n = 10, variant=\"base\", score_function_parameter=None):\n",
    "    \"\"\"Do multiple a* algorithms to get an average mean score\"\"\"\n",
    "    \n",
    "    t = time.perf_counter()\n",
    "    times = np.zeros(n_simulations)\n",
    "    t_start = t\n",
    "    \n",
    "    for k in range(n_simulations):\n",
    "        result = a_star(start_set, goal, models, max_iteration, similarity_choice=similarity_choice, prints=False, \n",
    "                        preprocess_rhs = preprocess_rhs, find_multiple_paths=False, shedding=shedding, \n",
    "                        shedding_n = shedding_n, variant=variant, score_function_parameter=score_function_parameter) \n",
    "        # check if we did not finish\n",
    "        #if 'Did not finish' in result:\n",
    "        #    print(f\"Couldn't finish one of the simulations in {max_iteration} iterations.\")\n",
    "            \n",
    "        times[k] = time.perf_counter() - t_start\n",
    "        t_start = time.perf_counter()\n",
    "\n",
    "    # if we got through the whole loop\n",
    "    t_avg = round(np.mean(times),5)\n",
    "    CI_half_length = round(1.96 * np.std(times, ddof=1) / np.sqrt(n_simulations),10) # ddof=1 to get sample standard deviation (normalize by N-1)\n",
    "    print(f\"Average time to simulate: {t_avg} ± {CI_half_length}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phdenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
