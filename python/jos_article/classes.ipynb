{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "breeding-mediterranean",
   "metadata": {},
   "source": [
    "# Implementation of framework\n",
    "Implementation of the metadata framework with an A* algorithm based on score functions. The notebook contains:\n",
    "- Class definitions and functions (including score function), for describing the metadata problem.\n",
    "- Examples of usage of the classes to illustrate the problem.\n",
    "- A* implementation. \n",
    "\n",
    "First, import the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "closed-contrast",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "#!{sys.executable} -m pip install ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "optical-reset",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib as plt\n",
    "import networkx as nx\n",
    "import copy as copy\n",
    "import numpy as np\n",
    "import re\n",
    "import itertools\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quick-lingerie",
   "metadata": {},
   "source": [
    "# Class definitions\n",
    "This part of the notebook contains all classes and functions for describing the problem. For more information about the meanings behind each class, see the accompanying presentation and paper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "soviet-oxide",
   "metadata": {},
   "source": [
    "### Not Initialised Error\n",
    "To signal the user about situations where an object has not yet been initialised. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "common-hotel",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NotInitialisedError(Exception):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "useful-theater",
   "metadata": {},
   "source": [
    "### Variable\n",
    "The smallest object in the problem. Each dataset contains multiple variables. Variables can have different levels of granularity. To change from one granularity to another, a conversion or aggregation is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "capable-tunnel",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Variable:\n",
    "    def __init__(self, name, granularity):\n",
    "        self.name = name\n",
    "        self.granularity = granularity\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(self.name) + str(self.granularity)\n",
    "    \n",
    "    def __eq__(self, other: \"Variable\"):\n",
    "        # compare self Variable object to the other Variable object\n",
    "        # they are equal if the name and granularity are equal\n",
    "        \n",
    "        return all([self.name == other.name, \n",
    "                    self.granularity == other.granularity])\n",
    "    \n",
    "    def __hash__(self):\n",
    "        # required for usage in sets. Since the str() of self is unique and contains all elements for equality, we use this for the hash.\n",
    "        return(hash(str(self)))\n",
    "    \n",
    "    def equal_name(self, other: \"Variable\"):\n",
    "        return self.name == other.name\n",
    "    \n",
    "    def get_name(self):\n",
    "        return self.name\n",
    "    \n",
    "    def get_granularity(self):\n",
    "        return self.granularity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virtual-snowboard",
   "metadata": {},
   "source": [
    "### Data\n",
    "A data source consists of a set of left-hand variables, a set of right-hand variables and context. The context is largely ignored for the modelling week. The similarity() method is the subject of the modelling week assignment. It is also used in the SetOfSources class, where the individual similarity scores of each data set are combined into a single value.\n",
    "\n",
    "The assignment of the modelling week is as follows: find a similarity score function that provides a small value for when two data sources have few variables in common, and a larger value when more variables are in common. One disadvantage of the current method is that sources with a large number of variables have a higher similarity score than sources with a smaller number of variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "narrative-tokyo",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Data(object):\n",
    "    def __init__(self, left_variables, right_variables, context):\n",
    "        self.left_variables = set(left_variables)\n",
    "        self.right_variables = set(right_variables)\n",
    "        self.context = set(context)  # ensure context is a set (argument may be a list)\n",
    "        self.path_step = \"none\"\n",
    "        self.score = False \n",
    "\n",
    "    def __str__(self):\n",
    "        left_str_separate = [str(v) for v in self.left_variables]\n",
    "        left_str_separate.sort()\n",
    "        left_str = \", \".join(left_str_separate)\n",
    "        right_str_separate = [str(v) for v in self.right_variables]\n",
    "        right_str_separate.sort()\n",
    "        right_str = \", \".join(right_str_separate)\n",
    "        full_str = \"(\" + left_str + \" | \" + right_str + \")\" + \"_\" + str(sorted([str(c) for c in self.context]))\n",
    "        return full_str\n",
    "    \n",
    "    def __eq__(self, other: \"Data\"):\n",
    "        # compare self Data object to the other Data object\n",
    "        # they are equal if all left- and right- sets of variables, and the context, are equal\n",
    "        \n",
    "        # use all for efficient evaluation (stops at the first element that is False)\n",
    "        return all([self.left_variables == other.left_variables,\n",
    "                    self.right_variables == other.right_variables,\n",
    "                    self.context == other.context])\n",
    "    \n",
    "    def __hash__(self):\n",
    "        # required for usage in sets. Since the str() of self is unique and contains all elements for equality, we use this for the hash.\n",
    "        return hash(str(self))\n",
    "    \n",
    "    def equal_nocontext(self, other: \"Data\"):\n",
    "        # compare self Data object to the other Data object\n",
    "        # similar to __eq__() except that this function does not care about context\n",
    "        # they are considered equal if all left- and right- sets of variables, are equal\n",
    "        \n",
    "        # use all for efficient evaluation (stops at the first element that is False)\n",
    "        return all([self.left_variables == other.left_variables,\n",
    "                    self.right_variables == other.right_variables])\n",
    "    \n",
    "    def get_context(self):\n",
    "        return self.context  # set of contexts\n",
    "    \n",
    "    def get_variable_names_left(self):\n",
    "        return {v.get_name() for v in self.left_variables}\n",
    "    \n",
    "    def contains_var_left(self, v_name):\n",
    "        return v_name in self.get_variable_names_left()\n",
    "        \n",
    "    def get_variable_names_right(self):    \n",
    "        return {v.get_name() for v in self.right_variables}\n",
    "    \n",
    "    def contains_var_right(self, v_name):\n",
    "        return v_name in self.get_variable_names_right()\n",
    "    \n",
    "    def get_variables_left(self):\n",
    "        return self.left_variables\n",
    "    \n",
    "    def get_variables_right(self):\n",
    "        return self.right_variables\n",
    "    \n",
    "    def reset_score(self):\n",
    "        # Always make sure to reset the score when making a (deep) copy of a dataset, or if you adjust any variables\n",
    "        # FUTURE TODO: for future speed up of the search, it would be nice to create a lookuptable of datasets and their scores\n",
    "        # when multiple paths to the same dataset are found through an aggregate_variable or convert_variable, the score\n",
    "        # will be reset and recalculated even though it is actually the same source, just with a different path. In that\n",
    "        # case, it would be nice to give preference to the source with the shorter path.  \n",
    "        self.score = False\n",
    "\n",
    "    def convert_variable(self, var_remove, var_add):\n",
    "        # beware: the check if this conversion is allowed should be executed before this method is used\n",
    "        self.left_variables.remove(var_remove)\n",
    "        self.left_variables.add(var_add)\n",
    "        self.score = False\n",
    "        \n",
    "    def aggregate_variable(self, var_remove, var_add):\n",
    "        # beware: the check if this aggregation is allowed should be executed before this method is used\n",
    "        self.right_variables.remove(var_remove)\n",
    "        self.right_variables.add(var_add)\n",
    "        self.score = False\n",
    "    \n",
    "    def similarity(self, other: \"Data\", variant = \"base\", prints=False,\n",
    "                   weight_right_sim = 1, weight_right_eq = 5, weight_left_sim = 2, weight_left_eq = 5,\n",
    "                   weight_context = 5):\n",
    "        # other is the goal Data\n",
    "        \n",
    "        if not self.score: # only calculate the score if it is not known yet\n",
    "\n",
    "            n_goal_vars_left = len(self.left_variables)\n",
    "            \n",
    "            # old weights:\n",
    "            #weight_right_eq = 2 * weight_right_sim\n",
    "            #weight_left_sim = n_goal_vars_left * weight_right_eq\n",
    "            #weight_left_eq = 2 * weight_left_sim\n",
    "            #weight_context = 2 * weight_left_eq\n",
    "            \n",
    "            if prints: \n",
    "                print(\"Calculating Score for a new data set: \"+str(self)+\" and other (goal): \"+str(other))\n",
    "                \n",
    "            left_equal = len(set(self.left_variables).intersection(other.left_variables))  # number of variables with equal name and granularity\n",
    "            right_equal = len(set(self.right_variables).intersection(other.right_variables))  # number of variables with equal name and granularity\n",
    "            \n",
    "            left_similar = len(self.get_variable_names_left().intersection(other.get_variable_names_left()))  # number of variables with equal name (those with equal granularity are counted again, so keep this in mind when setting weights)\n",
    "            left_similar -= left_equal    # remove double-counting\n",
    "            right_similar = len(self.get_variable_names_right().intersection(other.get_variable_names_right()))  # number of variables with equal name (those with equal granularity are counted again, so keep this in mind when setting weights)\n",
    "            right_similar -= right_equal  # remove double-counting\n",
    "        \n",
    "            context_score = weight_context*(self.context == other.context)\n",
    "            \n",
    "            left_equal_max = len(set(other.left_variables))  # number of variables with equal name and granularity\n",
    "            right_equal_max = len(set(other.right_variables))  # number of variables with equal name and granularity\n",
    "\n",
    "            base_score = sum([weight_left_eq*left_equal, weight_left_sim*left_similar,\n",
    "                            weight_right_eq*right_equal, weight_right_sim*right_similar,\n",
    "                            context_score])\n",
    "            \n",
    "            \n",
    "            if variant == \"base\":\n",
    "                # simply sum the multiplications of the variable weights\n",
    "                self.score = base_score\n",
    "            elif variant == \"base_coupled\":\n",
    "                # simply sum the multiplications of the variable weights\n",
    "                self.score = (sum([weight_left_eq*left_equal, weight_left_sim*left_similar]) *\n",
    "                        sum([weight_right_eq*right_equal, weight_right_sim*right_similar, context_score]))\n",
    "            elif variant == \"individual\":\n",
    "                # named \"likeness\" in the paper\n",
    "                # this may be a little slower because of the for loop, but it will keep the algorithm from adding unneccesary variables to datasets\n",
    "                # This similarity function is assymetric. It assumes that other is the goal and self is a dataset from one of the stages in the algorithm\n",
    "                score_tmp = 0\n",
    "                # left hand side\n",
    "                    \n",
    "                for goal_v_l in other.get_variables_left():\n",
    "                    if goal_v_l in self.left_variables: \n",
    "                        # an exact match on granularity is present\n",
    "                        score_tmp += weight_left_eq\n",
    "                    elif goal_v_l.get_name() in self.get_variable_names_left():\n",
    "                        # similar match on variable name (wihtout granularity)\n",
    "                        score_tmp += weight_left_sim\n",
    "                # right hand side\n",
    "                for goal_v_r in other.get_variables_right():\n",
    "                    if goal_v_r in self.right_variables: \n",
    "                        # an exact match on granularity is present\n",
    "                        score_tmp += weight_right_eq\n",
    "                    elif goal_v_r.get_name() in self.get_variable_names_right():\n",
    "                        # similar match on variable name  (wihtout granularity)\n",
    "                        score_tmp += weight_right_sim\n",
    "                if other.get_context == self.get_context:\n",
    "                    score_tmp += context_score\n",
    "\n",
    "                # penalize sources that have more variables in them \n",
    "                score_tmp = score_tmp / (len(set(self.get_variables_left())) + len(set(self.get_variables_right())))\n",
    "                \n",
    "                self.score = score_tmp\n",
    "\n",
    "            elif variant == \"normalized\":   # this is the default\n",
    "                # normalize score:\n",
    "                # by dividing by the maximum score that could be acchieved based on the number of variables in this source\n",
    "                # large sources gain a higher penalty\n",
    "                self.score = base_score / sum([weight_left_eq * left_equal_max, weight_right_eq * right_equal_max, weight_context])\n",
    "            \n",
    "            elif variant == \"normalized_coupled\":\n",
    "                # normalize score, but with rhs and lhs dependently (multiply instead of sum)\n",
    "                self.score = ((sum([weight_left_eq*left_equal, weight_left_sim*left_similar]) *\n",
    "                        sum([weight_right_eq*right_equal, weight_right_sim*right_similar, context_score])) /  \n",
    "                        (weight_left_eq * left_equal_max * (weight_right_eq * right_equal_max + weight_context)))\n",
    "\n",
    "            if prints: \n",
    "                print(\"score: \"+str(self.score)) \n",
    "            \n",
    "        return self.score\n",
    "    \n",
    "    def get_neighbours(self, agg = True):\n",
    "        # based on conversion and aggregation, give all unique datasets that can be created from datasource self, with exactly one manipulation\n",
    "        \n",
    "        # conversion\n",
    "        neighbours = set()\n",
    "        for v in self.left_variables:\n",
    "            # for each of the left variables, it can be converted to one of its connected granularities in the conversion graph\n",
    "            conversion_graph = ConversionGraph.get(v.get_name())\n",
    "            connected_granularities = conversion_graph.all_conversions(v.get_granularity())\n",
    "            for g in connected_granularities:\n",
    "                v2 = Variable(name=v.get_name(), granularity = g)  # copy the name, but use new granularity\n",
    "                data_temp = copy.deepcopy(self)  # copy of the current data set\n",
    "                data_temp.convert_variable(var_remove = v, var_add = v2)  # apply conversion (we have checked that it is valid when creating connected_granularities)\n",
    "                data_temp.path_step = \"convert (\"+str(v) + \" to \" + str(v2) + \"): \" + str(data_temp)\n",
    "                neighbours.add(data_temp)\n",
    "        \n",
    "        # aggregation \n",
    "        if agg:\n",
    "            for v in self.right_variables:\n",
    "                # for each of the left variables, it can be converted to one of its connected granularities in the conversion graph\n",
    "                aggregation_graph = AggregationGraph.get(v.get_name())\n",
    "                connected_granularities = aggregation_graph.all_aggregations(v.get_granularity())\n",
    "                for g in connected_granularities:\n",
    "                    v2 = Variable(name=v.get_name(), granularity = g)  # copy the name, but use new granularity\n",
    "                    data_temp = copy.deepcopy(self)\n",
    "                    data_temp.aggregate_variable(var_remove = v, var_add = v2)\n",
    "                    data_temp.path_step = \"aggregate (\"+str(v) + \" to \" + str(v2) + \"): \" + str(data_temp)\n",
    "                    neighbours.add(data_temp)\n",
    "        \n",
    "        # combination is not relevant when looking at a single data source, because two sources are always required for combining\n",
    "        \n",
    "        return neighbours\n",
    "    \n",
    "    def shrink(self, other: \"Data\"):  ###NEW###\n",
    "        \"\"\"\n",
    "        Returns True if self can be 'shrinked' into other. \n",
    "\n",
    "        (Temporary) solution to a combination issue where:\n",
    "\n",
    "        (a1, b3 | a3, c1)_I  +  (b2, e1 | a3, c1)_I  ->  (a1, b2, b3, e1 | a3, c1)_I\n",
    "\n",
    "        results in both b2 and b3 being in the dataset. The subdataset() function checks if a data set (goal) is within another dataset. Returns true/false. This is also relevant to the case:\n",
    "\n",
    "        (a1, b3 | a3, c1)_I  +  (b3, e1 | a3, c1)_I  ->  (a1, b3, e1 | a3, c1)_I\n",
    "\n",
    "        where the goal is a subset of the result of combining (a1, e1 | a3, c1)_I.\n",
    "\n",
    "        Left-hand variables can be dropped at any time without messing up the structure of the data set. \n",
    "        Right-hand variables can not simply be dropped. If dropped, duplicate units might exist in the \n",
    "        dataset because part of the unit descriptor is suddenly missing. Because of this, the sets of \n",
    "        right-hand variables must be equal. \n",
    "        \"\"\"\n",
    "    \n",
    "        return all([other.left_variables.issubset(self.left_variables),\n",
    "                self.right_variables == other.right_variables,\n",
    "                other.context.issubset(self.context)]) \n",
    "    \n",
    "    def shrink_nocontext(self, other: \"Data\"):  ###NEW###\n",
    "        \"\"\"\n",
    "        Same as shrink() but without the constraint that contexts must be equal\n",
    "        \"\"\"\n",
    "    \n",
    "        return all([other.left_variables.issubset(self.left_variables),\n",
    "                self.right_variables == other.right_variables])  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "careful-presentation",
   "metadata": {},
   "source": [
    "### Conversion Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "pretty-symphony",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ConversionGraph:\n",
    "    instances = []  # class attribute to keep track of class instances\n",
    "    \n",
    "    def __init__(self, variable_name, granularities, conversion_edges):\n",
    "        self.variable_name = variable_name\n",
    "        self.Graph = nx.Graph()\n",
    "        self.granularities = granularities\n",
    "\n",
    "        for g in granularities:\n",
    "            self.Graph.add_node(g)\n",
    "\n",
    "        for e in conversion_edges:\n",
    "            self.Graph.add_edge(*e)  # * unpacks edge tuple\n",
    "        \n",
    "        # add self to list of instances\n",
    "        if self.is_initialised(variable_name):\n",
    "            # an instance with this variable name was already known, remove that instance (so the new instance is the only one)\n",
    "            ConversionGraph.instances.remove(self.get(variable_name))\n",
    "            warnings.warn(\"Overwriting the ConversionGraph for variable \"+str(variable_name)+\"!\")\n",
    "            \n",
    "        ConversionGraph.instances.append(self)  # append instance to list of class instances\n",
    "        \n",
    "    @classmethod \n",
    "    def get(cls: \"ConversionGraph\", var_name):\n",
    "        # return the instance of this class for which the value variable_name is equal to var_name\n",
    "        # each ConversionGraph object should exist exactly once for each variable_name\n",
    "        \n",
    "        # first we make a list of \"all\" instances that statisfy the desired variable name\n",
    "        list_form = [inst for inst in cls.instances if inst.variable_name == var_name]\n",
    "        if len(list_form) > 0:\n",
    "            # list is not empty, so return the first element (there should only be one)\n",
    "            return list_form[0]\n",
    "           \n",
    "        else:\n",
    "            # no instance was found\n",
    "            raise NotInitialisedError(\"ConversionGraph \" + var_name)\n",
    "            \n",
    "    def get_max_granularities(self):\n",
    "        return max(self.granularities)\n",
    "        \n",
    "    def is_initialised(cls: \"ConversionGraph\", var_name):\n",
    "        list_form = [inst for inst in cls.instances if inst.variable_name == var_name]\n",
    "        return len(list_form) > 0\n",
    "\n",
    "    def add_granularity(self, new_granularity):\n",
    "        self.Graph.add_node(new_granularity)\n",
    "\n",
    "    def add_conversion_edge(self, new_edge):\n",
    "        self.Graph.add_edge(*new_edge)\n",
    "\n",
    "    def plot_graph(self):\n",
    "        nx.draw(self.Graph, with_labels=True, node_color=\"lightgrey\")\n",
    "        \n",
    "    def check_conversion(self, granularity_from, granularity_to):\n",
    "        # true: if there is a conversion path between granularity_from to granularity_to\n",
    "        # false: otherwise\n",
    "        return nx.has_path(self.Graph, granularity_from, granularity_to)\n",
    "    \n",
    "    def all_conversions(self, granularity_from):\n",
    "        # returns all possible granularities that can be reached from the granularity_from\n",
    "        connected_set = nx.node_connected_component(self.Graph, granularity_from)  # this includes the starting node\n",
    "        connected_set.remove(granularity_from)  # exclude starting node\n",
    "        return connected_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tested-preview",
   "metadata": {},
   "source": [
    "### Aggregation Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "hindu-invention",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AggregationGraph:\n",
    "    instances = []  # class attribute to keep track of class instances\n",
    "    \n",
    "    def __init__(self, variable_name, granularities, aggregation_edges):\n",
    "        self.variable_name = variable_name\n",
    "        self.Graph = nx.DiGraph()\n",
    "        self.granularities = granularities\n",
    "\n",
    "        for g in granularities:\n",
    "            self.Graph.add_node(g)\n",
    "\n",
    "        for e in aggregation_edges:\n",
    "            self.Graph.add_edge(*e)  # * unpacks edge tuple\n",
    "            \n",
    "         # add self to list of instances\n",
    "        if self.is_initialised(variable_name):\n",
    "            # an instance with this variable name was already known, remove that instance (so the new instance is the only one)\n",
    "            AggregationGraph.instances.remove(self.get(variable_name))\n",
    "            warnings.warn(\"Overwriting the AggregationGraph for variable \"+str(variable_name)+\"!\")\n",
    "        AggregationGraph.instances.append(self)  # append instance to list of class instances\n",
    "            \n",
    "    @classmethod \n",
    "    def get(cls: \"AggregationGraph\", var_name):\n",
    "        # return the instance of this class for which the value variable_name is equal to var_name\n",
    "        # each AggregationGraph object should exist exactly once for each variable_name\n",
    "\n",
    "        # first we make a list of \"all\" instances that statisfy the desired variable name\n",
    "        list_form = [inst for inst in cls.instances if inst.variable_name == var_name]\n",
    "        if len(list_form) > 0:\n",
    "            # list is not empty, so return the first element (there should only be one)\n",
    "            return list_form[0]\n",
    "        else:\n",
    "            # no instance was found\n",
    "            raise NotInitialisedError(\"AggregationGraph \" + var_name)\n",
    "            \n",
    "    def is_initialised(cls: \"AggregationGraph\", var_name):\n",
    "        list_form = [inst for inst in cls.instances if inst.variable_name == var_name]\n",
    "        return len(list_form) > 0\n",
    "\n",
    "    def get_max_granularities(self):\n",
    "        return max(self.granularities)        \n",
    "\n",
    "    def add_granularity(self, new_granularity):\n",
    "        self.Graph.add_node(new_granularity)\n",
    "\n",
    "    def add_conversion_edge(self, new_edge):\n",
    "        self.Graph.add_edge(*new_edge)  # * unpacks edge tuple\n",
    "\n",
    "    def plot_graph(self): \n",
    "        nx.draw(self.Graph, with_labels=True, node_color=\"lightgrey\")\n",
    "        \n",
    "    def check_aggregation(self, granularity_from, granularity_to):\n",
    "        # true: if there is an aggregation path between granularity_from to granularity_to\n",
    "        # false: otherwise\n",
    "        return nx.has_path(self.Graph, granularity_from, granularity_to)\n",
    "    \n",
    "    def get_all_granularities(self):\n",
    "        return self.granularities    \n",
    "\n",
    "    def all_aggregations(self, granularity_from):\n",
    "        # returns all possible granularities that can be reached from the granularity_from\n",
    "        reacheable_set = nx.descendants(self.Graph, granularity_from) \n",
    "        \n",
    "        return reacheable_set\n",
    "        \n",
    "    def all_aggregations_reversed(self, granularity_to):\n",
    "        # returns all possible granularities from which the granularity_from can be reached \n",
    "        reacheable_set = nx.ancestors(self.Graph, granularity_to) \n",
    "        \n",
    "        return reacheable_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decimal-silence",
   "metadata": {},
   "source": [
    "### Combining \n",
    "Combining depends on two data sources. It is allowed when their right variables are equal. Column wise combining: when the contexts of both input sources have some overlap (non-empty intersection), the union of all left variables is available in the new data source, for the intersection of the context. Row wise combining: when the contexts of both sources have no overlap (empty intersection), the intersection of the left variables is available in the new data source, but the new context is the union of the context of the two input sources. Sometimes, both row-wise and column-wise combination are possible, resulting into two different outcomes. The combines() function checks both options and will return a tuple with te row- and column-wise combination results respectively. If a combination is not possible, the result will be False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "separated-chemical",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combines(data1: Data, data2: Data):\n",
    "    # Create rowwise and colwise (in case combinations are not possible)\n",
    "    rowwise, colwise = False, False\n",
    "    \n",
    "    # Combinations are only possible if the right-hand side variables are equal\n",
    "    if data1.right_variables == data2.right_variables:\n",
    "        # The result will have the same right-hand side variables \n",
    "        right3 = data1.right_variables\n",
    "       \n",
    "        # row-wise combination\n",
    "        if set(data1.left_variables) & set(data2.left_variables):\n",
    "            # there is overlap between the left variables \n",
    "            \n",
    "            # row-wise merge possible     \n",
    "            # no overlap between context of both sources, so only the same left-hand side variables can be merged\n",
    "            left3 = set(data1.left_variables).intersection(set(data2.left_variables))  # intersection of L1 and L2\n",
    "            context3 = data1.context.union(data2.context)  # union of C1 and C3\n",
    "            \n",
    "            rowwise = Data(right_variables = right3, left_variables = left3, context = context3)\n",
    "            \n",
    "        # column-wise combination \n",
    "        if data1.context & data2.context:\n",
    "            # set1 & set2: checks if there exists an intersection between two sets\n",
    "        \n",
    "            # column-wise merge possible\n",
    "            # context of both input sources have overlap, so the left-hand variables can be merged\n",
    "            left3 = set(data1.left_variables).union(set(data2.left_variables))  # union of L1 and L2\n",
    "            context3 = data1.context.intersection(data2.context)  # intersection of C1 and C2\n",
    "            \n",
    "            colwise = Data(right_variables = right3, left_variables = left3, context = context3)\n",
    "        \n",
    "    return rowwise, colwise\n",
    "        \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mental-machinery",
   "metadata": {},
   "source": [
    "### Set of Sources\n",
    "Two variants of the similarity score function are implemented: the sum and max of the individual data scores from the data source similarity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "finished-array",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SetOfSources:\n",
    "    def __init__(self, start_set):\n",
    "        self.set_of_sources = set(start_set)\n",
    "        self.path = [\"start_set\"]  # for keeping track of the path that created the current set\n",
    "        self.tree = []  # for keeping track of which iterations of the algorithm added to this path\n",
    "        self.score = False \n",
    "        \n",
    "    def __str__(self):\n",
    "        full_str = \"{\" + \",\\n \".join([str(d) for d in self.set_of_sources]) + \"\\n}\"\n",
    "        return full_str\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        # check if this set equals the other set\n",
    "        # since the set contains Data objects, which have a __eq__() method, it suffices to rely on that method\n",
    "        # considered equal if the sets have the same data Data objects, \n",
    "        # regardless of order in which the Data objects appear\n",
    "        return self.set_of_sources == other.set_of_sources\n",
    "    \n",
    "    def add_data_source(self, data_new: Data, part_of_path=\"NA\", iteration=\"-1\"):\n",
    "        self.set_of_sources = self.set_of_sources.union({data_new})\n",
    "        self.add_to_path(part_of_path)\n",
    "        self.tree.append(iteration)\n",
    "        self.score = False  # reset score because of change in the set of sources\n",
    "        \n",
    "    def add_to_path(self, part_of_path: str):\n",
    "        # For keeping track of the path (in words).\n",
    "        self.path.append(part_of_path)\n",
    "        \n",
    "    def contains(self, data_source: Data): \n",
    "        if any([data_source == d for d in self.set_of_sources]):\n",
    "            return True\n",
    "        else:\n",
    "            # check if any of the data sources in self can be shrinked into data_source\n",
    "            # this takes longer to compute, so only do this step when data_source is not exactly in self\n",
    "            return any([data_in_self.shrink(data_source) for data_in_self in self.set_of_sources])\n",
    "            \n",
    "    def contains_nocontext(self, data_source: Data): ###NEW### (updated)\n",
    "        # Same as contains() except here, we do NOT care about the context \n",
    "        # This is used for some models\n",
    "        \n",
    "        matching_sources = [ds for ds in self.set_of_sources if ds.equal_nocontext(data_source)]\n",
    "        \n",
    "        if data_source in self.set_of_sources:\n",
    "            return matching_sources\n",
    "        else:\n",
    "            # check if any of the data sources in self can be shrinked into data_source\n",
    "            # this takes longer to compute, so only do this step when data_source is not exactly in self\n",
    "            \n",
    "            matching_sources = [data_in_self for data_in_self in self.set_of_sources if data_in_self.shrink_nocontext(data_source)]\n",
    "            return matching_sources\n",
    "        \n",
    "        \n",
    "    def data_sources_with_var_left(self, v_name):\n",
    "        # TODO this code (from the students) can be sped up by list comprehension\n",
    "        data_sources = set()\n",
    "        \n",
    "        for d in self.set_of_sources:\n",
    "            if d.contains_var_left(v_name):\n",
    "                data_sources.add(d)\n",
    "                \n",
    "        return data_sources\n",
    "    \n",
    "    def data_sources_with_var_right(self, v_name):\n",
    "        data_sources = set()\n",
    "        \n",
    "        for d in self.set_of_sources:\n",
    "            if d.contains_var_right(v_name):\n",
    "                data_sources.add(d)\n",
    "                \n",
    "        return data_sources\n",
    "            \n",
    "    def similarity_sum(self, goal_data: Data, variant=\"base\"):\n",
    "        # only calculate the score if it was not saved from a previous iteration\n",
    "        if not self.score: \n",
    "            self.score = sum((goal_data.similarity(d, variant=variant) for d in self.set_of_sources))\n",
    "        return self.score\n",
    "    \n",
    "    def similarity_topsum(self, goal_data: Data, multiplier=3, variant=\"base\", prints=False):\n",
    "        # only calculate the score if it was not saved from a previous iteration\n",
    "        # take the sum of the N highest scores of datasets in the set of sources\n",
    "        # N is determined by 3*the number of variables of the goal\n",
    "        # This score function is designed to combat the effect of rewarding a large number of datasources in the \n",
    "        # set of sources, which similarity_sum() suffers from. \n",
    "        # In exeptional cases, more than three manipulations are needed per variable in the goal set. If you think you \n",
    "        # may be in one of these cases, you can gradually increase the multiplier if small values don't succeed.\n",
    "        if not self.score: \n",
    "            all_scores = np.array([d.similarity(goal_data, variant=variant, prints=prints) for d in self.set_of_sources])  # scores per dataset\n",
    "            n_select = multiplier*(len(goal_data.get_variables_left()) + len(goal_data.get_variables_right()))  # number ofo var\n",
    "            self.score = all_scores[np.argsort(all_scores)][-n_select:].sum()  # sort scores and sum over N highest\n",
    "        return self.score  \n",
    "    \n",
    "    def similarity_max(self, goal_data: Data, variant=\"base\"):\n",
    "        if not self.score: \n",
    "            self.score = max((d.similarity(goal_data, variant=variant) for d in self.set_of_sources)) \n",
    "        return self.score \n",
    "    \n",
    "    def similarity_mean(self, goal_data: Data, variant=\"base\"):\n",
    "        if not self.score: \n",
    "            self.score = np.mean(list(d.similarity(goal_data, variant=variant) for d in self.set_of_sources))\n",
    "        return self.score \n",
    "\n",
    "    def similarity_median(self, goal_data: Data, variant=\"base\"):\n",
    "        if not self.score: \n",
    "            self.score = np.median(list(d.similarity(goal_data, variant=variant) for d in self.set_of_sources))\n",
    "        return self.score \n",
    "    \n",
    "    def similarity_min(self, goal_data: Data, variant=\"base\"):\n",
    "        if not self.score: \n",
    "            self.score = min((d.similarity(goal_data, variant=variant) for d in self.set_of_sources))\n",
    "        return self.score \n",
    "    \n",
    "    def similarity_minmax(self, goal_data: Data, variant=\"base\"):\n",
    "        if not self.score: \n",
    "            self.score = max((d.similarity(goal_data, variant=variant) for d in self.set_of_sources)) * (min((d.similarity(goal_data, variant=variant) for d in self.set_of_sources)))\n",
    "        return self.score \n",
    "    \n",
    "    def similarity_maxmean(self, goal_data: Data, variant=\"base\"):\n",
    "        if not self.score: \n",
    "            self.score = max((d.similarity(goal_data, variant=variant) for d in self.set_of_sources)) + np.mean(list(d.similarity(goal_data, variant=variant) for d in self.set_of_sources))\n",
    "        return self.score \n",
    "        \n",
    "    def similarity_maxmeanmin(self, goal_data: Data, variant=\"base\"):\n",
    "        if not self.score: \n",
    "            self.score = max((d.similarity(goal_data, variant=variant) for d in self.set_of_sources)) * np.mean(list(d.similarity(goal_data, variant=variant) for d in self.set_of_sources)) * min((goal_data.similarity(d, variant=variant) for d in self.set_of_sources))\n",
    "        return self.score \n",
    "    \n",
    "    def similarity_max_per_variable(self, goal_data: Data, variant=\"base\"):  \n",
    "        if not self.score: \n",
    "            idx = 0\n",
    "        \n",
    "            goal_contexts = goal_data.get_context()\n",
    "            maxs = np.zeros(len(goal_data.left_variables)*len(goal_contexts))\n",
    "            \n",
    "            for context in goal_contexts:\n",
    "                \n",
    "                for var in goal_data.left_variables:\n",
    "                    data_sources = self.data_sources_with_var_left(var.get_name())\n",
    "                    \n",
    "                    if data_sources:\n",
    "                        maxs[idx] = max((d.similarity(goal_data, variant=variant) for d in data_sources if context in d.get_context()))\n",
    "                    idx += 1\n",
    "                \n",
    "            self.score = np.mean(maxs)\n",
    "        return self.score    \n",
    "    \n",
    "    def similarity_max_per_variable_bonus(self, goal_data: Data, variant=\"base\"):\n",
    "        # Reimplemented from the students' version, disregarding context for now\n",
    "        \n",
    "        # TODO: currently not applicable to context\n",
    "        if not self.score: \n",
    "            idx = 0\n",
    "        \n",
    "            maxs = np.zeros(len(goal_data.left_variables))\n",
    "            maxs_data_sources = []\n",
    "            \n",
    "            for left_var in goal_data.left_variables:\n",
    "                # get all data sources in current_set that have the same left variable\n",
    "                data_sources_left_match = list(self.data_sources_with_var_left(left_var.get_name()))\n",
    "\n",
    "                if len(data_sources_left_match) > 0:\n",
    "                    # one or more data sources were found\n",
    "                    \n",
    "                    scores_list = [d.similarity(goal_data, variant=variant) for d in data_sources_left_match]\n",
    "                    max_idx = np.argmax(scores_list)\n",
    "                    max_data_source = data_sources_left_match[max_idx]\n",
    "                    maxs_data_sources.append(max_data_source.right_variables)\n",
    "                    maxs[idx] = scores_list[max_idx]\n",
    "                idx += 1\n",
    "                    \n",
    "            bonus_mult = len(set.intersection(*maxs_data_sources)) / len(set.union(*maxs_data_sources))\n",
    "            \n",
    "            self.score = np.mean(maxs) * (bonus_mult + 1)/2    # don't start at 0\n",
    "        return self.score \n",
    "                        \n",
    "    def get_sources(self):\n",
    "        return self.set_of_sources\n",
    "    \n",
    "    def get_neighbours(self, agg = True):\n",
    "        # based on conversion, aggregation and combination, give all unique datasets that can be created from the current set, with exactly one manipulation\n",
    "        \n",
    "        all_neighbours = set()\n",
    "        \n",
    "        # Conversion and aggregating\n",
    "        for d in self.set_of_sources:\n",
    "            # add all items in the set d.get_neighbours() to all_neighbours\n",
    "            # These neighbours come from the individual datasets (and already have their path noted). \n",
    "            all_neighbours.update(d.get_neighbours(agg))\n",
    "        \n",
    "        # Combination\n",
    "        set_of_sources_temp = list(self.set_of_sources)  # temporarily make the set of sources into a list, so the indices are fixed\n",
    "        \n",
    "        for i, j in zip(*np.triu_indices(len(set_of_sources_temp), k=1)):\n",
    "            # Loop through all combinations of (two) available data sources by using the indices of the upper \n",
    "            # triangle without diagonal (offset k=1) of a matrix of size n by n, where n = len(self.set_of_sources)\n",
    "        \n",
    "            combines_temp_row, combines_temp_col = combines(set_of_sources_temp[i], set_of_sources_temp[j])\n",
    "            if combines_temp_row:\n",
    "                # Rowwise combination was possible, so add result to neighbours\n",
    "                combines_temp_row.path_step = \"combine (rowwise): \" + str(combines_temp_row)\n",
    "                all_neighbours.add(combines_temp_row)  \n",
    "                \n",
    "            if combines_temp_col:\n",
    "                # Columnwise combination was possible, so add result to neighbours\n",
    "                combines_temp_col.path_step = \"combine (columnwise): \" + str(combines_temp_col)\n",
    "                all_neighbours.add(combines_temp_col) \n",
    "                        \n",
    "        return all_neighbours\n",
    "    \n",
    "\n",
    "    def get_neighbours_models(self, models = None):\n",
    "        # based on modelling, give all unique datasets that can be created from the current set, with exactly one modelling manipulation\n",
    "        # this will not return any neighbours that can be created through conversion, aggregation or combination\n",
    "        \n",
    "        all_neighbours = set()\n",
    "        \n",
    "        # Modelling\n",
    "        if models is not None:\n",
    "            # Since the case study only uses models with exactly two input sources, restrict the combinations\n",
    "            # to exactly two sources. The code below allows for all combinations of any number, but this increases\n",
    "            # the running time.\n",
    "            \n",
    "            # Get all (unique) subsets of all available sources (like a powerset, except for the emptyset and single sources)\n",
    "            #data_combos = list(subset_combos(set_of_sources_temp))\n",
    "            #for dataset_combo in data_combos:  # (then use this instead of the (i, j) loop)\n",
    "            \n",
    "            set_of_sources_temp = list(self.set_of_sources)  # temporarily make the set of sources into a list, so the indices are fixed\n",
    "        \n",
    "            for i, j in zip(*np.triu_indices(len(set_of_sources_temp), k=1)):\n",
    "                # Loop through all combinations of (two) available data sources by using the indices of the upper \n",
    "                # triangle without diagonal (offset k=1) of a matrix of size n by n, where n = len(self.set_of_sources)\n",
    "\n",
    "                # Loop through all combinations of available data sources and see if any models are applicable\n",
    "                for model_tmp in models:\n",
    "                    # If the model is applicable, the list of output sources will be returned. Note: multiple outputs\n",
    "                    # are possible. \n",
    "                    model_output = model_tmp.apply(potential_input = [set_of_sources_temp[i], set_of_sources_temp[j]])\n",
    "                    if model_output != False:\n",
    "                        # The model was applicable and returned output. Add this output to the list of all neighbours.\n",
    "                        # When unapplicable the value of model_output is False.\n",
    "                        for mo in model_output:\n",
    "                            mo.path_step = \"model (\" + model_tmp.name + \"):\" + str(mo)\n",
    "                \n",
    "                        all_neighbours.update(set(model_output))\n",
    "                        \n",
    "        return all_neighbours\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "alternate-choir",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For creating test cases with a starting set of sources, goal, graphs and models\n",
    "class TestCase:\n",
    "    def __init__(self, goal, start_set, graphs, models=None):\n",
    "        self.goal = goal\n",
    "        self.start_set = start_set\n",
    "        self.graphs = graphs\n",
    "        self.models = models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "matched-shoulder",
   "metadata": {},
   "source": [
    "### Modelling\n",
    "Modelling is currently implemented as exceptions to the rules of the other manipulations. Each available model, however trivial, must be specified. A model is based on a set of input data and output data. If the input data is available, then the output data can be acchieved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "brilliant-tiffany",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, input_data, output_data, context_rule):\n",
    "        self.input_data = set(input_data)  # can be multiple data sources\n",
    "        self.output_data = output_data\n",
    "        self.context_rule = context_rule\n",
    "        self.name = \"Unnamed\"\n",
    "\n",
    "    def __str__(self):\n",
    "        input_str = \" + \".join([str(x) for x in self.input_data])\n",
    "    \n",
    "        return self.name + \": \" + input_str + \" -> \" + str(self.output_data)\n",
    "            \n",
    "    def apply(self, potential_input):\n",
    "        # Note: many context rules can be thought of. If they get so specific that the relation between sources and context matter, \n",
    "        # the intention is to write a child class that overwrites the apply() function for the specific case\n",
    "            \n",
    "        # If each source in the required input (self.input_data) is present in the potential_input, then the model is applicable\n",
    "        # It gets a bit tricky to check this, because sometimes, the context is less strictly required then other times\n",
    "        \n",
    "        if self.context_rule == \"exact\":\n",
    "            # Check if the all input sources are available in the potential input, with exact context matches\n",
    "            \n",
    "            if all([input_source in potential_input for input_source in self.input_data]):\n",
    "                # based on exact matches\n",
    "                return [self.output_data]\n",
    "            else:\n",
    "                # if for all required data sources, there is a source in the potential data that can be shrinked into the required data source, \n",
    "                # the model can also be applied\n",
    "                results = []\n",
    "                for required_data in self.input_data:\n",
    "                    results.append(any([pi.shrink(required_data) for pi in potential_input]))\n",
    "                    \n",
    "                # check if shrinking was enough to satisfy requirements\n",
    "                if all(results):\n",
    "                    return [self.output_data]\n",
    "                else:\n",
    "                    return False\n",
    "        \n",
    "        elif self.context_rule in [\"intersection\", \"union\", \"equal\"]:\n",
    "            # For these context_rule's we'll need to do some set manipulation to find out if the model requirements are met\n",
    "            \n",
    "            output_list = []  # here we will add any outcomes for the inputs that satisfy the context requirements\n",
    "            context_matches = []  # list of lists \n",
    "            \n",
    "            for input_data_temp in self.input_data:\n",
    "                # for each required input_data source, check if there are matches in potential_input. If so, add their contexts to the list.\n",
    "                context_matches_temp = [ds.get_context() for ds in potential_input if ds.equal_nocontext(input_data_temp)]\n",
    "                context_matches.append(context_matches_temp)\n",
    "            \n",
    "            if all(len(context_matches_temp)>0 for context_matches_temp in context_matches):\n",
    "                # For all required input sources, at least one available data source was found\n",
    "                \n",
    "                for context_permutation in itertools.product(*context_matches):\n",
    "                    # Note: * unpacks the list context_matches once. This results in context_permutations being a tuple of the same length\n",
    "                    # as the number of required input sources.\n",
    "                    \n",
    "                    if self.context_rule == \"intersection\":\n",
    "                        # The context of the output_data is the intersection of all contexts of the input data. \n",
    "                        # If no intersection is possible, the model cannot be applied\n",
    "            \n",
    "                        # Use * to unpack the tuple context_permutaiton, and then we calculate the intersection of the various contexts\n",
    "                        context_intersection = set.intersection(*context_permutation)\n",
    "                        if len(context_intersection) > 0:\n",
    "                            # If this resulted in a non-empty set, we have found a permutation that works! \n",
    "                            new_context = context_intersection\n",
    "                            \n",
    "                    elif self.context_rule == \"union\":\n",
    "                        # Use * to unpack the tuple context_permutaiton, and then we calculate the union of the various contexts\n",
    "                        context_union = set.union(*context_permutation)\n",
    "                        if len(context_union) > 0:\n",
    "                            # If this resulted in a non-empty set, we have found a permutation that works! \n",
    "                            new_context = context_union\n",
    "                            \n",
    "                    elif self.context_rule == \"equal\":\n",
    "                        context_union = set.union(*context_permutation)  # calculate union of all context\n",
    "                        \n",
    "                        if all([cp == context_union for cp in context_permutation]):\n",
    "                            # If the union of all sets are equal to all of the sets, then all sets are equal\n",
    "                            new_context = context_union\n",
    "        \n",
    "                    # Now, we can generate an output of the model.\n",
    "                    output_data_temp = copy.deepcopy(self.output_data)  # copy the output_data\n",
    "                    output_data_temp.context = new_context  # overwrite the context \n",
    "                    output_list.append(output_data_temp)\n",
    "\n",
    "                if len(output_list) > 0:\n",
    "                    # One ore more results were found, these can now be returned\n",
    "                    return output_list\n",
    "            else:\n",
    "                return False\n",
    "                 \n",
    "        else: \n",
    "            warnings.warn(\"Modelling: the rules regarding context were not clear\")\n",
    "            return False\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bab131f",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "83b778e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_combos(s):\n",
    "    # start at range 2 because we do not need the emptyset or single sources\n",
    "    return [combo for r in range(2, len(s) + 1) for combo in itertools.combinations(s, r)]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phdenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
